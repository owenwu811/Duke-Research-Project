{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_same(items):\n",
    "    if isinstance(items[0], float) and all(math.isnan(x) for x in items):\n",
    "        return True\n",
    "    return all(x == items[0] for x in items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_df = pd.read_excel('NESCent_publications/NESCent_CV.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = turk_df.groupby('Input.name').agg(lambda x: x.tolist()[0] if all_same(x.tolist()) else x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_urls = ['http://notfound.com', 'http://none', 'http://NA', 'http://na.com', 'http://NA@NA.com', 'http://none.com', 'http://www.NA@NA.com', 'https://NA', 'https://none.com', 'https://notfound.com', 'http://notexactlyfound.com', 'http://notexplicityfound.com', 'http://www.thereisnocv.com', 'file:///C:/Users/user/Downloads/Documents/Ove_Nilsson.pdf', 'httpys://ocs.yale.edu/sites/default/files/files/CV%20to%20ResumeWorkshopfinal.pdf' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_urls = []\n",
    "\n",
    "for index, row in turk_grouped_df.iterrows():\n",
    "    urls = row['Answer.web_url']\n",
    "    \n",
    "    # Create array of all urls\n",
    "    parsed_urls = []\n",
    "    if isinstance(urls, str):\n",
    "        if urls[-1] == '|':\n",
    "            urls = urls[0:-1]\n",
    "        \n",
    "        cleaned_url = [x for x in urls.split('|')]\n",
    "        parsed_urls.extend( cleaned_url)\n",
    "\n",
    "    elif isinstance(urls, list):\n",
    "        for url in urls:\n",
    "            if url[-1] == '|':\n",
    "                url = url[0:-1]\n",
    "            parsed_urls.extend(url.split('|'))\n",
    "        \n",
    "    # Remove 'https://', 'http://', 'www.' for comparison\n",
    "    clean_urls = []\n",
    "    for index, url in enumerate(parsed_urls):\n",
    "        cleaned_url = url\n",
    "        if cleaned_url not in stop_urls:\n",
    "            clean_urls.append(cleaned_url)\n",
    "    \n",
    "    all_clean_urls.append(clean_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_counters = [Counter(x) for x in all_clean_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = [[], [], [], [], []]\n",
    "\n",
    "for counter in url_counters:\n",
    "    top_urls = [x[0] for x in counter.most_common()]\n",
    "    \n",
    "    for index, url in enumerate(top_urls):\n",
    "        top_5[index].append(url)\n",
    "        \n",
    "    for index in range(len(top_urls), 5):\n",
    "        top_5[index].append(-1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, url_group in enumerate(top_5):\n",
    "    turk_grouped_df['top_' + str(index+1) + '_url'] =  url_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = turk_grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = turk_grouped_df.drop(['Title', 'Description', 'Input.searchlink', 'Answer.web_url'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_scraper import get_text_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "raw = [[], [], [], [], []]\n",
    "\n",
    "with tqdm(total=180) as pbar:\n",
    "    for i, row in turk_grouped_df.iterrows():\n",
    "        pbar.update(1)\n",
    "        for j in range(0,5):\n",
    "            url = row['top_' + str(j+1) + '_url']\n",
    "\n",
    "            if url == -1:\n",
    "                raw[j].append(-1)\n",
    "            else:\n",
    "                raw[j].append(get_text_from_url(url, False).replace('\\n', \" \"))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, raw_group in enumerate(raw):\n",
    "    turk_grouped_df['top_' + str(index+1) + '_raw'] =  raw_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df.to_csv('turked_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns True for: (2015) (2013). 2013.\n",
    "def isCitationYear(test_string):\n",
    "    if re.findall('^\\(\\d{4}\\)', test_string):\n",
    "        return True\n",
    "    \n",
    "    if re.findall('^\\d{4}\\.', test_string):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCitationYear('(2015):')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tolerance = 15\n",
    "\n",
    "all_publications = []\n",
    "\n",
    "for i, row in turk_grouped_df.iterrows():\n",
    "    publications = set()\n",
    "    \n",
    "    for j in range(0,5):\n",
    "        raw = row['top_' + str(j+1) + '_raw']\n",
    "        \n",
    "        if raw == -1:\n",
    "            break\n",
    "            \n",
    "        last_name = row['Input.name'].split()[1]\n",
    "\n",
    "        raw_split = raw.split()\n",
    "        \n",
    "        for k, word in enumerate(raw_split):\n",
    "            if last_name in word and k+1 < len(raw_split):\n",
    "                current_index = k+1\n",
    "                current_word = raw_split[current_index]\n",
    "                found = True\n",
    "                while(not isCitationYear(current_word)):\n",
    "                    current_index += 1\n",
    "                    \n",
    "                    try:\n",
    "                        current_word = raw_split[current_index]\n",
    "                    except IndexError:\n",
    "                        found = False\n",
    "                        break\n",
    "                        \n",
    "                    if current_index == tolerance:\n",
    "                        found = False\n",
    "                        break\n",
    "                        \n",
    "                if found and current_index+1 != len(raw_split):\n",
    "                    title = ''\n",
    "                    current_index += 1\n",
    "                    current = raw_split[current_index]\n",
    "                    while True:\n",
    "                        title += ' ' + current\n",
    "\n",
    "                        if '.' in current or '?' in current:\n",
    "                            break\n",
    "                        \n",
    "                        current_index += 1\n",
    "                    \n",
    "                        \n",
    "                        try:\n",
    "                            current = raw_split[current_index]\n",
    "                        except:\n",
    "                            found = False\n",
    "                            break\n",
    "                    \n",
    "                    if found:\n",
    "                        publications.add(title)           \n",
    "    all_publications.append(publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_publications_clean = ['' if x=='set()' else ', '.join(str(y) for y in x) for x in all_publications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df['unconfirmed_publications'] = all_publications_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "turk_grouped_df.to_csv('turk_grouped_publications.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df.to_excel('turk_grouped_publications.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = pd.read_csv('turk_grouped_publications.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_initials_df = pd.read_excel('turk_grouped_publications_with_middle_initial.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df[\"Google Scholar Middle Initial\"] = middle_initials_df[\"Google Scholar Middle Initial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_with_middle_initial = turk_grouped_df[turk_grouped_df[\"Google Scholar Middle Initial\"].notnull() ]\n",
    "turked_grouped_without_middle_initial = turk_grouped_df[turk_grouped_df[\"Google Scholar Middle Initial\"].isnull() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_with_middle_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_without_middle_initial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_with_middle_initial.to_csv('turk_grouped_with_middle_initial_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_without_middle_initial.to_csv('turk_grouped_without_middle_initial_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
