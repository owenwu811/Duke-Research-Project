{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import math\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_same(items):\n",
    "    if isinstance(items[0], float) and all(math.isnan(x) for x in items):\n",
    "        return True\n",
    "    return all(x == items[0] for x in items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_df = pd.read_excel('NESCent_publications/NESCent_CV.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = turk_df.groupby('Input.name').agg(lambda x: x.tolist()[0] if all_same(x.tolist()) else x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_urls = ['http://notfound.com', 'http://none', 'http://NA', 'http://na.com', 'http://NA@NA.com', 'http://none.com', 'http://www.NA@NA.com', 'https://NA', 'https://none.com', 'https://notfound.com', 'http://notexactlyfound.com', 'http://notexplicityfound.com', 'http://www.thereisnocv.com', 'file:///C:/Users/user/Downloads/Documents/Ove_Nilsson.pdf', 'httpys://ocs.yale.edu/sites/default/files/files/CV%20to%20ResumeWorkshopfinal.pdf' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clean_urls = []\n",
    "\n",
    "for index, row in turk_grouped_df.iterrows():\n",
    "    urls = row['Answer.web_url']\n",
    "    \n",
    "    # Create array of all urls\n",
    "    parsed_urls = []\n",
    "    if isinstance(urls, str):\n",
    "        if urls[-1] == '|':\n",
    "            urls = urls[0:-1]\n",
    "        \n",
    "        cleaned_url = [x for x in urls.split('|')]\n",
    "        parsed_urls.extend( cleaned_url)\n",
    "\n",
    "    elif isinstance(urls, list):\n",
    "        for url in urls:\n",
    "            if url[-1] == '|':\n",
    "                url = url[0:-1]\n",
    "            parsed_urls.extend(url.split('|'))\n",
    "        \n",
    "    # Remove 'https://', 'http://', 'www.' for comparison\n",
    "    clean_urls = []\n",
    "    for index, url in enumerate(parsed_urls):\n",
    "        cleaned_url = url\n",
    "        if cleaned_url not in stop_urls:\n",
    "            clean_urls.append(cleaned_url)\n",
    "    \n",
    "    all_clean_urls.append(clean_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_counters = [Counter(x) for x in all_clean_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = [[], [], [], [], []]\n",
    "\n",
    "for counter in url_counters:\n",
    "    top_urls = [x[0] for x in counter.most_common()]\n",
    "    \n",
    "    for index, url in enumerate(top_urls):\n",
    "        top_5[index].append(url)\n",
    "        \n",
    "    for index in range(len(top_urls), 5):\n",
    "        top_5[index].append(-1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, url_group in enumerate(top_5):\n",
    "    turk_grouped_df['top_' + str(index+1) + '_url'] =  url_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = turk_grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df = turk_grouped_df.drop(['Title', 'Description', 'Input.searchlink', 'Answer.web_url'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_scraper import get_text_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "raw = [[], [], [], [], []]\n",
    "\n",
    "with tqdm(total=180) as pbar:\n",
    "    for i, row in turk_grouped_df.iterrows():\n",
    "        pbar.update(1)\n",
    "        for j in range(0,5):\n",
    "            url = row['top_' + str(j+1) + '_url']\n",
    "\n",
    "            if url == -1:\n",
    "                raw[j].append(-1)\n",
    "            else:\n",
    "                raw[j].append(get_text_from_url(url, False).replace('\\n', \" \"))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, raw_group in enumerate(raw):\n",
    "    turk_grouped_df['top_' + str(index+1) + '_raw'] =  raw_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df.to_csv('turked_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns True for: (2015) (2013). 2013.\n",
    "def isCitationYear(test_string):\n",
    "    if re.findall('^\\(\\d{4}\\)', test_string):\n",
    "        return True\n",
    "    \n",
    "    if re.findall('^\\d{4}\\.', test_string):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCitationYear('(2015):')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tolerance = 30\n",
    "\n",
    "all_publications = []\n",
    "\n",
    "for i, row in turk_grouped_df.iterrows():\n",
    "    publications = set()\n",
    "    \n",
    "    for j in range(0,5):\n",
    "        raw = row['top_' + str(j+1) + '_raw']\n",
    "        \n",
    "        if raw == -1:\n",
    "            break\n",
    "            \n",
    "        last_name = row['Input.name'].split()[1]\n",
    "\n",
    "        raw_split = raw.split()\n",
    "        \n",
    "        for k, word in enumerate(raw_split):\n",
    "            if last_name in word and k+1 < len(raw_split):\n",
    "                current_index = k+1\n",
    "                current_word = raw_split[current_index]\n",
    "                found = True\n",
    "                while(not isCitationYear(current_word)):\n",
    "                    current_index += 1\n",
    "                    \n",
    "                    try:\n",
    "                        current_word = raw_split[current_index]\n",
    "                    except IndexError:\n",
    "                        found = False\n",
    "                        break\n",
    "                        \n",
    "                    if current_index == tolerance:\n",
    "                        found = False\n",
    "                        break\n",
    "                        \n",
    "                if found and current_index+1 != len(raw_split):\n",
    "                    title = ''\n",
    "                    current_index += 1\n",
    "                    current = raw_split[current_index]\n",
    "                    while True:\n",
    "                        title += ' ' + current\n",
    "\n",
    "                        if '.' in current or '?' in current:\n",
    "                            break\n",
    "                        \n",
    "                        current_index += 1\n",
    "                    \n",
    "                        \n",
    "                        try:\n",
    "                            current = raw_split[current_index]\n",
    "                        except:\n",
    "                            found = False\n",
    "                            break\n",
    "                    \n",
    "                    if found:\n",
    "                        publications.add(title)           \n",
    "    all_publications.append(publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_publications_clean = ['' if x=='set()' else ', '.join(str(y) for y in x) for x in all_publications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df['unconfirmed_publications'] = all_publications_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "turk_grouped_df.to_csv('turk_grouped_publications.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df.to_excel('turk_grouped_publications.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_initials_df = pd.read_excel('turk_grouped_publications_with_middle_initial.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df[\"Google Scholar Middle Initial\"] = middle_initials_df[\"Google Scholar Middle Initial\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_with_middle_initial = turk_grouped_df[turk_grouped_df[\"Google Scholar Middle Initial\"].notnull() ]\n",
    "turked_grouped_with_middle_initial.to_csv('turk_grouped_with_middle_initial_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_wo_middle_initial = turk_grouped_df[turk_grouped_df[\"Google Scholar Middle Initial\"].isnull() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turked_grouped_wo_middle_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turk_grouped_df.to_csv(\"turk_grouped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "turk_grouped_df = pd.read_csv(\"Stage_2/turk_grouped_with_middle_initial_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in turk_grouped_df[0:1].iterrows():\n",
    "    test = row['Google Scholar Middle Initial']\n",
    "    search_query = scholarly.search_pubs_query(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('export http_proxy=\"http://localhost:8123\"')\n",
    "os.system('export https_proxy=\"https://localhost:8123\"')\n",
    "'Congratulations' in requests.get('http://check.torproject.org/').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = next(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1_fill = result1.fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(result1_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scholarly\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = scholarly.search_pubs_query(\"AG Grant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(vars(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'lum-customer-hl_ed3aa9cc-zone-static'\n",
    "password = 'ly6gws5xff8h'\n",
    "port = 22225\n",
    "session_id = random.random()\n",
    "super_proxy_url = ('http://%s-country-us-session-%s:%s@zproxy.lum-superproxy.io:%d' %\n",
    "        (username, session_id, password, port))\n",
    "proxyDict = { \n",
    "              \"http\"  : super_proxy_url, \n",
    "              \"https\" : super_proxy_url, \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = \"https://youtube.com\"\n",
    "requests.get(url, proxies=proxyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "print('If you get error \"ImportError: No module named \\'six\\'\" install six:\\n'+\\\n",
    "    '$ sudo pip install six');\n",
    "print('To enable your free eval account and get CUSTOMER, YOURZONE and ' + \\\n",
    "    'YOURPASS, please contact sales@luminati.io')\n",
    "import sys\n",
    "if sys.version_info[0]==2:\n",
    "    import six\n",
    "    from six.moves.urllib import request\n",
    "    opener = request.build_opener(\n",
    "        request.ProxyHandler(\n",
    "            {'http': 'http://lum-customer-davidcheng-zone-static:ly6gws5xff8h@zproxy.lum-superproxy.io:22225'}))\n",
    "    print(opener.open('http://lumtest.com/myip.json').read())\n",
    "if sys.version_info[0]==3:\n",
    "    import urllib.request\n",
    "    opener = urllib.request.build_opener(\n",
    "        urllib.request.ProxyHandler(\n",
    "            {'http': 'http://lum-customer-davidcheng-zone-static:ly6gws5xff8h@zproxy.lum-superproxy.io:22225'}))\n",
    "    print(opener.open('http://lumtest.com/myip.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "print('If you get error \"ImportError: No module named \\'six\\'\" install six:\\n'+\\\n",
    "    '$ sudo pip install six');\n",
    "print('To enable your free eval account and get CUSTOMER, YOURZONE and ' + \\\n",
    "    'YOURPASS, please contact sales@luminati.io')\n",
    "import sys\n",
    "if sys.version_info[0]==2:\n",
    "    import six\n",
    "    from six.moves.urllib import request\n",
    "    opener = request.build_opener(\n",
    "        request.ProxyHandler(\n",
    "            {'http': 'http://lum-customer-davidcheng-zone-static:ly6gws5xff8h@zproxy.lum-superproxy.io:22225'}))\n",
    "    print(opener.open('http://lumtest.com/myip.json').read())\n",
    "if sys.version_info[0]==3:\n",
    "    import urllib.request\n",
    "    opener = urllib.request.build_opener(\n",
    "        urllib.request.ProxyHandler(\n",
    "            {'http': 'http://lum-customer-davidcheng-zone-static:ly6gws5xff8h@zproxy.lum-superproxy.io:22225'}))\n",
    "    print(opener.open('http://lumtest.com/myip.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring\n",
    "from itertools import cycle\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    "\n",
    "\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(proxy_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    " \n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(url)\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies\n",
    " \n",
    " \n",
    "#If you are copy pasting proxy ips, put in the list below\n",
    "#proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "proxies = get_proxies()\n",
    "proxy_pool = cycle(proxies)\n",
    " \n",
    "url = 'https://httpbin.org/ip'\n",
    "for i in range(1,11):\n",
    "    #Get a proxy from the pool\n",
    "    proxy = next(proxy_pool)\n",
    "    print(\"Request #%d\"%i)\n",
    "    try:\n",
    "        response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "        print(response.json())\n",
    "    except:\n",
    "        #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "        #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "        print(\"Skipping. Connnection error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
